{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Pytorch with Examples\n",
    "Link: https://pytorch.org/tutorials/beginner/pytorch_with_examples.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement network using numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 25716667.52609746\n",
      "1 19208177.704846527\n",
      "2 16925943.95623912\n",
      "3 16272030.967289142\n",
      "4 15878929.02195695\n",
      "5 14850644.977473713\n",
      "6 12929828.898457605\n",
      "7 10337548.982875062\n",
      "8 7657868.410944873\n",
      "9 5338349.85868625\n",
      "10 3600140.0721950885\n",
      "11 2400177.7480116785\n",
      "12 1619074.7050179483\n",
      "13 1121304.188425892\n",
      "14 805270.9911233713\n",
      "15 601757.1061896046\n",
      "16 467371.89580057084\n",
      "17 375469.57068296394\n",
      "18 310064.3187146545\n",
      "19 261644.77971179516\n",
      "20 224405.71628490213\n",
      "21 194787.32017049214\n",
      "22 170621.17864025338\n",
      "23 150457.99786424398\n",
      "24 133372.89996470942\n",
      "25 118703.46856779745\n",
      "26 106082.03748094616\n",
      "27 95063.85310081358\n",
      "28 85413.35493955816\n",
      "29 76908.65095178381\n",
      "30 69380.54256433577\n",
      "31 62696.69779072444\n",
      "32 56741.915902059\n",
      "33 51427.93130881807\n",
      "34 46668.81535363033\n",
      "35 42402.53191379833\n",
      "36 38570.702339576565\n",
      "37 35124.07463997612\n",
      "38 32019.279518322994\n",
      "39 29214.585801878013\n",
      "40 26680.011543587676\n",
      "41 24388.38941146013\n",
      "42 22312.872962570338\n",
      "43 20430.893371042137\n",
      "44 18722.376326000613\n",
      "45 17169.115307704094\n",
      "46 15755.34615591324\n",
      "47 14467.117437207606\n",
      "48 13292.223458785027\n",
      "49 12220.366299275685\n",
      "50 11241.261870596816\n",
      "51 10346.262160366741\n",
      "52 9527.662240606885\n",
      "53 8778.342074883927\n",
      "54 8091.9750321042175\n",
      "55 7462.798730904156\n",
      "56 6885.680072172877\n",
      "57 6356.173216734005\n",
      "58 5869.730251777879\n",
      "59 5423.018798614166\n",
      "60 5012.064520280899\n",
      "61 4633.913276268949\n",
      "62 4286.160741829526\n",
      "63 3965.9213898574817\n",
      "64 3670.8017974462764\n",
      "65 3399.004444747483\n",
      "66 3148.471192762515\n",
      "67 2917.453926012974\n",
      "68 2704.094423989958\n",
      "69 2507.240363070557\n",
      "70 2325.3345893425994\n",
      "71 2157.281586590655\n",
      "72 2001.9524263668209\n",
      "73 1858.4674354846256\n",
      "74 1726.2339894978948\n",
      "75 1603.8757212343148\n",
      "76 1490.7405922567887\n",
      "77 1385.8923201090647\n",
      "78 1288.7167079306773\n",
      "79 1198.6348436426147\n",
      "80 1115.146316061297\n",
      "81 1037.7115598742635\n",
      "82 965.8924085981303\n",
      "83 899.2544491700005\n",
      "84 837.4342242648578\n",
      "85 780.0371742583444\n",
      "86 726.7116300391785\n",
      "87 677.1922667147292\n",
      "88 631.1775212817311\n",
      "89 588.3936106795884\n",
      "90 548.625108559762\n",
      "91 511.6409157220835\n",
      "92 477.25576661495575\n",
      "93 445.2758229076604\n",
      "94 415.49993059875226\n",
      "95 387.7907364133471\n",
      "96 362.0107841557692\n",
      "97 337.99306179123454\n",
      "98 315.61991939172026\n",
      "99 294.7779555478172\n",
      "100 275.3592829064622\n",
      "101 257.2783924128531\n",
      "102 240.41005763522423\n",
      "103 224.68506501443653\n",
      "104 210.0212826460555\n",
      "105 196.34606964421658\n",
      "106 183.59088276392185\n",
      "107 171.68717292607175\n",
      "108 160.5842456737932\n",
      "109 150.21951995086795\n",
      "110 140.5469193757911\n",
      "111 131.51382702021886\n",
      "112 123.07545431888754\n",
      "113 115.19648555913037\n",
      "114 107.83495597241594\n",
      "115 100.96432981362958\n",
      "116 94.5414219706835\n",
      "117 88.53921260528114\n",
      "118 82.92651453507438\n",
      "119 77.6784126080886\n",
      "120 72.77325512422395\n",
      "121 68.18548295035941\n",
      "122 63.89665583966633\n",
      "123 59.883406260471276\n",
      "124 56.1275399714047\n",
      "125 52.614948597431535\n",
      "126 49.32672829206941\n",
      "127 46.24903431211638\n",
      "128 43.37134824767841\n",
      "129 40.674507076817065\n",
      "130 38.150262170349414\n",
      "131 35.785889904968165\n",
      "132 33.571264211699464\n",
      "133 31.497615956929756\n",
      "134 29.556021640939754\n",
      "135 27.735959957437647\n",
      "136 26.030665431093645\n",
      "137 24.432466872730096\n",
      "138 22.9347922046005\n",
      "139 21.531090407740383\n",
      "140 20.215702320999338\n",
      "141 18.981897842033895\n",
      "142 17.825040619697813\n",
      "143 16.739878624420548\n",
      "144 15.722082150153122\n",
      "145 14.76787899552885\n",
      "146 13.87310600423059\n",
      "147 13.033270531586387\n",
      "148 12.245058082391989\n",
      "149 11.505499476213721\n",
      "150 10.81198338661758\n",
      "151 10.161086403988207\n",
      "152 9.549771881437739\n",
      "153 8.976008790224146\n",
      "154 8.43733121796484\n",
      "155 7.931553299382413\n",
      "156 7.457022965064771\n",
      "157 7.01120772202339\n",
      "158 6.592566173523981\n",
      "159 6.199288418996609\n",
      "160 5.82998168875443\n",
      "161 5.483276483286286\n",
      "162 5.157293878844579\n",
      "163 4.850962577560148\n",
      "164 4.563229823009485\n",
      "165 4.292899619570549\n",
      "166 4.038767640862325\n",
      "167 3.799930017648478\n",
      "168 3.575346660758114\n",
      "169 3.3642922844090464\n",
      "170 3.1660300048931767\n",
      "171 2.9795435922703217\n",
      "172 2.8041851355046896\n",
      "173 2.6393033202784077\n",
      "174 2.484292160779864\n",
      "175 2.3384930060443736\n",
      "176 2.201395605793864\n",
      "177 2.0724230761284836\n",
      "178 1.9511737249159742\n",
      "179 1.837069449182209\n",
      "180 1.729713191248012\n",
      "181 1.6287128507106436\n",
      "182 1.5337404822924703\n",
      "183 1.444368179773038\n",
      "184 1.360292418114493\n",
      "185 1.2811303594349774\n",
      "186 1.206650681681297\n",
      "187 1.136531350828769\n",
      "188 1.0705436962628716\n",
      "189 1.008436883560538\n",
      "190 0.9500038910753317\n",
      "191 0.8949787196895815\n",
      "192 0.8431810523899264\n",
      "193 0.7944092813793553\n",
      "194 0.7484941400574731\n",
      "195 0.7052588618661029\n",
      "196 0.6645535439589401\n",
      "197 0.6262464975872708\n",
      "198 0.5901522048965333\n",
      "199 0.5561538668722337\n",
      "200 0.5241467055974076\n",
      "201 0.4939985941200864\n",
      "202 0.4655964014451425\n",
      "203 0.438852585141302\n",
      "204 0.41366393877680374\n",
      "205 0.38993127208442824\n",
      "206 0.3675760289001769\n",
      "207 0.3465135192017713\n",
      "208 0.32666609095723564\n",
      "209 0.307966722389787\n",
      "210 0.2903552687944967\n",
      "211 0.27375894610602547\n",
      "212 0.2581151403195514\n",
      "213 0.24337844313904317\n",
      "214 0.2294857773036571\n",
      "215 0.21639749376141698\n",
      "216 0.20406082347102417\n",
      "217 0.19243340505142578\n",
      "218 0.1814808759424694\n",
      "219 0.17115291494273283\n",
      "220 0.1614143683289988\n",
      "221 0.15223778617876255\n",
      "222 0.1435856150128476\n",
      "223 0.1354290876551798\n",
      "224 0.12774008643160228\n",
      "225 0.12049301495558595\n",
      "226 0.11366069867788799\n",
      "227 0.1072176858097832\n",
      "228 0.10114098696302212\n",
      "229 0.09541415658012932\n",
      "230 0.09001090397712432\n",
      "231 0.08491702687446998\n",
      "232 0.08011538468884061\n",
      "233 0.0755856756926418\n",
      "234 0.07131354584731814\n",
      "235 0.06728473932104342\n",
      "236 0.06348628272692264\n",
      "237 0.05990196428985815\n",
      "238 0.056522508081152986\n",
      "239 0.05333533072432525\n",
      "240 0.05032906989629176\n",
      "241 0.047492536270404215\n",
      "242 0.04481689513928871\n",
      "243 0.04229355392888963\n",
      "244 0.03991295492543742\n",
      "245 0.03766753200126641\n",
      "246 0.035548953807548064\n",
      "247 0.03355052442056281\n",
      "248 0.03166469092457226\n",
      "249 0.029885584980382532\n",
      "250 0.028207077599983206\n",
      "251 0.026623182942664633\n",
      "252 0.025128995739073484\n",
      "253 0.023718953095876554\n",
      "254 0.02238866084785863\n",
      "255 0.021133457483287262\n",
      "256 0.01994878409157246\n",
      "257 0.018831004521977397\n",
      "258 0.017776100770908427\n",
      "259 0.016780692975984134\n",
      "260 0.015841454575262114\n",
      "261 0.014954934322083292\n",
      "262 0.014118252855153291\n",
      "263 0.013328891605602626\n",
      "264 0.01258363128139729\n",
      "265 0.011880209510072173\n",
      "266 0.011216234743161923\n",
      "267 0.010589637286721862\n",
      "268 0.009998303626834442\n",
      "269 0.009440038474568873\n",
      "270 0.008913032073298497\n",
      "271 0.008415780261094415\n",
      "272 0.007946281371544253\n",
      "273 0.00750301605053305\n",
      "274 0.007084623299631923\n",
      "275 0.006689774200018033\n",
      "276 0.006316957809543085\n",
      "277 0.0059649262969504934\n",
      "278 0.005632706164465315\n",
      "279 0.005319198057513608\n",
      "280 0.005023059466858727\n",
      "281 0.004743441240144771\n",
      "282 0.0044795007867799155\n",
      "283 0.004230328647606954\n",
      "284 0.003995008276123981\n",
      "285 0.0037728320264659785\n",
      "286 0.003563083490008471\n",
      "287 0.0033650956069934375\n",
      "288 0.003178055316119021\n",
      "289 0.0030015020058205347\n",
      "290 0.0028348039893353596\n",
      "291 0.0026773779695158763\n",
      "292 0.002528698835090019\n",
      "293 0.002388325830205491\n",
      "294 0.00225581859785244\n",
      "295 0.0021306681907429467\n",
      "296 0.002012457842914035\n",
      "297 0.0019008544148964312\n",
      "298 0.0017954555895761575\n",
      "299 0.0016959015947212905\n",
      "300 0.0016018865969754628\n",
      "301 0.0015131146208306801\n",
      "302 0.0014292802491691773\n",
      "303 0.0013501056608005398\n",
      "304 0.0012753337548846238\n",
      "305 0.001204713075550127\n",
      "306 0.001138006747642999\n",
      "307 0.0010750069257589004\n",
      "308 0.0010155116728284305\n",
      "309 0.0009593242535782943\n",
      "310 0.0009062392475058868\n",
      "311 0.0008561215127176098\n",
      "312 0.000808767834429694\n",
      "313 0.0007640502169676585\n",
      "314 0.0007217986084385009\n",
      "315 0.0006818928114889137\n",
      "316 0.000644198106345254\n",
      "317 0.0006086009986893837\n",
      "318 0.0005749781220554528\n",
      "319 0.0005432109013175304\n",
      "320 0.0005132107619069547\n",
      "321 0.0004848757214707376\n",
      "322 0.0004581013107770132\n",
      "323 0.00043280520290025636\n",
      "324 0.00040891260440569996\n",
      "325 0.0003863488123503579\n",
      "326 0.00036502440849867166\n",
      "327 0.00034488045193334577\n",
      "328 0.00032585591059149587\n",
      "329 0.00030788494974988125\n",
      "330 0.00029090123109995106\n",
      "331 0.00027485767976072307\n",
      "332 0.0002597051520102087\n",
      "333 0.0002453901191791693\n",
      "334 0.00023186183468848208\n",
      "335 0.00021908037486341732\n",
      "336 0.0002070077056430537\n",
      "337 0.0001956037508001636\n",
      "338 0.0001848266506689101\n",
      "339 0.00017464543889960377\n",
      "340 0.00016502646969174373\n",
      "341 0.00015593742718186213\n",
      "342 0.00014734934546169144\n",
      "343 0.0001392363633365672\n",
      "344 0.00013157254684865877\n",
      "345 0.00012433231187042597\n",
      "346 0.00011748959608221364\n",
      "347 0.00011102319430192714\n",
      "348 0.00010491410079311363\n",
      "349 9.914204631954719e-05\n",
      "350 9.368723571285649e-05\n",
      "351 8.853554205647088e-05\n",
      "352 8.366767227939583e-05\n",
      "353 7.906606982659709e-05\n",
      "354 7.471872470482356e-05\n",
      "355 7.061075881885194e-05\n",
      "356 6.672946391282774e-05\n",
      "357 6.306100998469997e-05\n",
      "358 5.9594872563157225e-05\n",
      "359 5.6319954654510026e-05\n",
      "360 5.32252306925718e-05\n",
      "361 5.0300618079663794e-05\n",
      "362 4.7536583284624255e-05\n",
      "363 4.492582737979645e-05\n",
      "364 4.2458252704775256e-05\n",
      "365 4.0127075428821775e-05\n",
      "366 3.7923243782657495e-05\n",
      "367 3.5840502536146156e-05\n",
      "368 3.387275880084917e-05\n",
      "369 3.201315880827611e-05\n",
      "370 3.025557510168096e-05\n",
      "371 2.859485950259123e-05\n",
      "372 2.7026055492427397e-05\n",
      "373 2.5542734852576576e-05\n",
      "374 2.4140941839991972e-05\n",
      "375 2.2816362260562524e-05\n",
      "376 2.156460000560037e-05\n",
      "377 2.038158781317752e-05\n",
      "378 1.9263667768279913e-05\n",
      "379 1.82070053484434e-05\n",
      "380 1.72085092778677e-05\n",
      "381 1.6264942776018602e-05\n",
      "382 1.5372917562562393e-05\n",
      "383 1.452998430519847e-05\n",
      "384 1.373364771888615e-05\n",
      "385 1.2980887464804599e-05\n",
      "386 1.2269246486993899e-05\n",
      "387 1.1596634247931707e-05\n",
      "388 1.0961110686052836e-05\n",
      "389 1.0360480572763896e-05\n",
      "390 9.792825590885443e-06\n",
      "391 9.256289921977587e-06\n",
      "392 8.749095764998012e-06\n",
      "393 8.269916552579552e-06\n",
      "394 7.816827881183852e-06\n",
      "395 7.388604926131908e-06\n",
      "396 6.983857314568273e-06\n",
      "397 6.601560292521389e-06\n",
      "398 6.24001413508892e-06\n",
      "399 5.898340562683133e-06\n",
      "400 5.5753471091150325e-06\n",
      "401 5.270149741626678e-06\n",
      "402 4.981630001737673e-06\n",
      "403 4.708955130057812e-06\n",
      "404 4.451219054661021e-06\n",
      "405 4.207589196571727e-06\n",
      "406 3.977318149438095e-06\n",
      "407 3.759632574133325e-06\n",
      "408 3.553922705540382e-06\n",
      "409 3.3595115406752374e-06\n",
      "410 3.175826350917741e-06\n",
      "411 3.0020590607196466e-06\n",
      "412 2.837800108142517e-06\n",
      "413 2.682565458343018e-06\n",
      "414 2.535840068651548e-06\n",
      "415 2.3971414027413295e-06\n",
      "416 2.2660468759855824e-06\n",
      "417 2.1421452366594177e-06\n",
      "418 2.0250373302856015e-06\n",
      "419 1.914287869757956e-06\n",
      "420 1.809612455817474e-06\n",
      "421 1.71066922978203e-06\n",
      "422 1.6171805619981056e-06\n",
      "423 1.5287685620267643e-06\n",
      "424 1.4452114386540303e-06\n",
      "425 1.3662160779126847e-06\n",
      "426 1.2915552823251906e-06\n",
      "427 1.2209827530415565e-06\n",
      "428 1.1542675842440673e-06\n",
      "429 1.091185774490218e-06\n",
      "430 1.0315592526777015e-06\n",
      "431 9.752029475092421e-07\n",
      "432 9.219209163616702e-07\n",
      "433 8.715475704170703e-07\n",
      "434 8.239479247385445e-07\n",
      "435 7.789483269074142e-07\n",
      "436 7.36403141927594e-07\n",
      "437 6.961776256274071e-07\n",
      "438 6.58152888435422e-07\n",
      "439 6.222156525999375e-07\n",
      "440 5.882464590603542e-07\n",
      "441 5.561221767157647e-07\n",
      "442 5.2575250336266e-07\n",
      "443 4.970477862190925e-07\n",
      "444 4.699136624420735e-07\n",
      "445 4.442645126929241e-07\n",
      "446 4.200184160629895e-07\n",
      "447 3.970901552996398e-07\n",
      "448 3.7541788320837255e-07\n",
      "449 3.5492391843300825e-07\n",
      "450 3.355505576912362e-07\n",
      "451 3.172359247463732e-07\n",
      "452 2.999335429281079e-07\n",
      "453 2.83564327826624e-07\n",
      "454 2.680912065460324e-07\n",
      "455 2.534630235530148e-07\n",
      "456 2.396345067502573e-07\n",
      "457 2.2656095272529951e-07\n",
      "458 2.1420469247518143e-07\n",
      "459 2.0251874753157482e-07\n",
      "460 1.91470142835122e-07\n",
      "461 1.8102775183657354e-07\n",
      "462 1.7115241750440195e-07\n",
      "463 1.618166820791787e-07\n",
      "464 1.5299640686289162e-07\n",
      "465 1.446573534114999e-07\n",
      "466 1.3676965290770378e-07\n",
      "467 1.2931023689500995e-07\n",
      "468 1.2225871851293392e-07\n",
      "469 1.1559275515981978e-07\n",
      "470 1.0929325507461336e-07\n",
      "471 1.0333363000560938e-07\n",
      "472 9.769972568333313e-08\n",
      "473 9.237343654852712e-08\n",
      "474 8.733932379520722e-08\n",
      "475 8.257877498207389e-08\n",
      "476 7.807868941880659e-08\n",
      "477 7.382276799535825e-08\n",
      "478 6.979931418997114e-08\n",
      "479 6.599544601026636e-08\n",
      "480 6.23984258494615e-08\n",
      "481 5.8998352343804145e-08\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "482 5.578399992979778e-08\n",
      "483 5.2745261823670356e-08\n",
      "484 4.9871505277556315e-08\n",
      "485 4.7153797343733485e-08\n",
      "486 4.4584651921765476e-08\n",
      "487 4.2156641242048154e-08\n",
      "488 3.986021536097982e-08\n",
      "489 3.768851781885028e-08\n",
      "490 3.563530462338084e-08\n",
      "491 3.3694232212118826e-08\n",
      "492 3.1859245484362996e-08\n",
      "493 3.012477742050367e-08\n",
      "494 2.848399562587672e-08\n",
      "495 2.6932562237083417e-08\n",
      "496 2.546603510441858e-08\n",
      "497 2.407923792464567e-08\n",
      "498 2.2767986085518317e-08\n",
      "499 2.152837651267707e-08\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# N is bash size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# create random input and output data\n",
    "x = np.random.randn(N, D_in)\n",
    "y = np.random.randn(N, D_out)\n",
    "\n",
    "# randomly initialize weights\n",
    "w1 = np.random.randn(D_in, H)\n",
    "w2 = np.random.randn(H, D_out)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "\n",
    "for t in range(500):\n",
    "    # Forward pass: compute predicted y\n",
    "    h = x.dot(w1)\n",
    "    h_relu = np.maximum(h, 0)\n",
    "    y_pred = h_relu.dot(w2)\n",
    "    \n",
    "    # compute and print loss\n",
    "    loss = np.square(y_pred - y).sum()\n",
    "    print(t, loss)\n",
    "    \n",
    "    # Backprop to compute gradient of w1 and w2 with respect to loss\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_w2 = h_relu.T.dot(grad_y_pred)\n",
    "    grad_h_relu = grad_y_pred.dot(w2.T)\n",
    "    grad_h = grad_h_relu.copy()\n",
    "    grad_h[h < 0] = 0\n",
    "    grad_w1 = x.T.dot(grad_h)\n",
    "    \n",
    "    # Update weights\n",
    "    w1 -= learning_rate * grad_w1\n",
    "    w2 -= learning_rate * grad_w2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 492.3892822265625\n",
      "199 2.3734118938446045\n",
      "299 0.016925575211644173\n",
      "399 0.00035549444146454334\n",
      "499 4.6073964767856523e-05\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "dtype = torch.float\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "# N is bash size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create random input and output data\n",
    "x = torch.randn(N, D_in, device=device, dtype=dtype)\n",
    "y = torch.randn(N, D_out, device=device, dtype=dtype)\n",
    "\n",
    "# Randomly initialize weights\n",
    "w1 = torch.randn(D_in, H, device=device, dtype=dtype)\n",
    "w2 = torch.randn(H, D_out, device=device, dtype=dtype)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(500):\n",
    "    # Forward pass: compute predicted y\n",
    "    h = x.mm(w1)\n",
    "    h_relu = h.clamp(min=0)\n",
    "    y_pred = h_relu.mm(w2)\n",
    "    \n",
    "    # Compute and print loss\n",
    "    loss = (y_pred - y).pow(2).sum().item()\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss)\n",
    "    \n",
    "    # Backprop to compute radients of w1 and w2 ith respect to loss\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_w2 = h_relu.t().mm(grad_y_pred)\n",
    "    grad_h_relu = grad_y_pred.mm(w2.t())\n",
    "    grad_h = grad_h_relu.clone()\n",
    "    grad_h[h < 0] = 0\n",
    "    grad_w1 = x.t().mm(grad_h)\n",
    "    \n",
    "    # upgrade weights using gradient descent\n",
    "    w1 -= learning_rate * grad_w1\n",
    "    w2 -= learning_rate * grad_w2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch: Tensors and autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 776.7689208984375\n",
      "199 5.674274444580078\n",
      "299 0.06827440112829208\n",
      "399 0.0012478386051952839\n",
      "499 0.00010865596414078027\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# N is bash size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create random Tensors to hold input and outputs.\n",
    "# Setting requires_grad=False indicates that we do not need to compute gradients\n",
    "# with the respect to these Tensors during backward pass\n",
    "x = torch.randn(N, D_in, device=device, dtype=dtype)\n",
    "y = torch.randn(N, D_out, device=device, dtype=dtype)\n",
    "\n",
    "# Create random Tensors for weights\n",
    "# Setting requires_grad=True indicates that we want to compute gradients with\n",
    "# respect to these Tensors during backward pass.\n",
    "w1 = torch.randn(D_in, H, device=device, dtype=dtype, requires_grad=True)\n",
    "w2 = torch.randn(H, D_out, device=device, dtype=dtype, requires_grad=True)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(500):\n",
    "    # Forward pass: compute predicted y using operations on Tensors; These\n",
    "    # are exactly the same operations we used to compute the forward pass using\n",
    "    # Tensors, but we do not need to keep references to ntermidiate values since\n",
    "    # we are not implementing the backward pass by hand.\n",
    "    \n",
    "    y_pred = x.mm(w1).clamp(min=0).mm(w2)\n",
    "    \n",
    "    # Compute and print loss using operations on Tensors.\n",
    "    # Now loss is a Tensor of shape (1,)\n",
    "    # loss.item() gets the scalar value held in loss.\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.item())\n",
    "    \n",
    "    # Use autograd to compute the backward pass. This call will compute the\n",
    "    # gradient of loss with respect to all Tensors with required_grad=True.\n",
    "    # After this call w1.grad and w2.grad will be Tensors hoding the gradient\n",
    "    # of the loss with respect to w1 and w2 respectively\n",
    "    loss.backward()\n",
    "    \n",
    "    # Manually update weights using gradient descent. Wrap in torch.no_grad()\n",
    "    # because weights have requires_grad=True, but we don't need to track this\n",
    "    # in autograd.\n",
    "    # An alternative way is to operate on weight.data and weight.grad.data.\n",
    "    # Recall that tensor.data gives a tensor that shares the storage with\n",
    "    # tensor, but doesn't track history.\n",
    "    # You can also use torch.optim.SGD to achieve this.\n",
    "    with torch.no_grad():\n",
    "        w1 -= learning_rate * w1.grad\n",
    "        w2 -= learning_rate * w2.grad\n",
    "\n",
    "        # Manually zero the gradients after updating weights\n",
    "        w1.grad.zero_()\n",
    "        w2.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch: Defining new autograd functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Torch\n",
    "\n",
    "class MyReLU(torch.autograd.Function):\n",
    "    \"\"\"\n",
    "    We can implement our own custom autograd Functions by subclassing\n",
    "    torch.autograd.Function and implementing the forawrd and backward passes\n",
    "    which operate on Tensors.\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        \"\"\"\n",
    "        In the forward pass we receive a Tensor containg the input and return\n",
    "        a Tensor containg the output. ctx is a context object that can be used\n",
    "        to stash information for backward computation. You can cache arbitrary\n",
    "        objects for use in the backward pass using the ctx.save_for_backward method.\n",
    "        \"\"\"\n",
    "        ctx.save_for_backward(input)\n",
    "        return input.clamp(min=0)\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
