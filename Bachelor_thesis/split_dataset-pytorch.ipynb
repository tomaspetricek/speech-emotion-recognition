{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "\n",
    "from config import DATASET_PATH\n",
    "from datasets import (Dataset, RAVDESSLabel, TESSLabel, \n",
    "                     EMOVOLabel, SAVEELabel, MFCCData, WAVData,\n",
    "                     RAVDESSUnifiedLabel, TESSUnifiedLabel, SAVEEUnifiedLabel,\n",
    "                     EMOVOUnifiedLabel)\n",
    "\n",
    "from tools import add_margin, IndexPicker\n",
    "\n",
    "from classifiers import Sequential"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ravdess_path = DATASET_PATH.format(language=\"english\", name=\"RAVDESS\", form=\"mfcc\")\n",
    "\n",
    "ravdess_mfcc_unified = Dataset(ravdess_path, MFCCData(), RAVDESSUnifiedLabel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tess_path = DATASET_PATH.format(language=\"english\", name=\"TESS\", form=\"mfcc\")\n",
    "\n",
    "tess_mfcc_unified = Dataset(tess_path, MFCCData(), TESSUnifiedLabel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "savee_path = DATASET_PATH.format(language=\"english\", name=\"SAVEE\", form=\"mfcc\")\n",
    "\n",
    "savee_mfcc_unified = Dataset(savee_path, MFCCData(), SAVEEUnifiedLabel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "emovo_path = DATASET_PATH.format(language=\"italian\", name=\"EMOVO\", form=\"mfcc\")\n",
    "\n",
    "emovo_mfcc_unified = Dataset(emovo_path, MFCCData(), EMOVOUnifiedLabel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ravdess_mfcc_unified.combine(savee_mfcc_unified, tess_mfcc_unified, emovo_mfcc_unified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ravdess_mfcc_unified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = dataset.samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(list(samples['coefficients']))\n",
    "\n",
    "y = np.array(list(samples['emotion']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = len(np.unique(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1282163,)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1282163, 39)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Margin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12 s, sys: 4.39 s, total: 16.4 s\n",
      "Wall time: 16.5 s\n"
     ]
    }
   ],
   "source": [
    "index_picker = IndexPicker(10, 10)\n",
    "%time X_margined = np.array(add_margin(X, index_picker))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1282163, 21, 39)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_margined.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reshape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.44 s, sys: 1.5 s, total: 3.94 s\n",
      "Wall time: 3.95 s\n"
     ]
    }
   ],
   "source": [
    "n_samples,window_length,n_features = X_margined.shape\n",
    "%time X_reshaped = np.array(np.reshape(X_margined, (n_samples, -1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1282163, 819)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_reshaped.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import TransformerMixin,BaseEstimator\n",
    "\n",
    "\n",
    "class Scaler(BaseEstimator,TransformerMixin):\n",
    "\n",
    "    def __init__(self, scaler):\n",
    "        self.scaler = scaler\n",
    "\n",
    "    def fit(self,X,y=None):\n",
    "        self.scaler.fit(X.reshape(-1, 1))\n",
    "        return self\n",
    "\n",
    "    def transform(self,X):\n",
    "        return self.scaler.transform(X.reshape(-1, 1)).reshape(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepere for model fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(X_reshaped, y, stratify=y, test_size=0.05)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train_full, y_train_full, stratify=y_train_full, test_size=0.05)\n",
    "\n",
    "# scaler = Scaler(StandardScaler())\n",
    "# %time X_train = scaler.fit_transform(X_train)    ## fit it only on train data\n",
    "# %time X_valid = scaler.transform(X_valid)\n",
    "# %time X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1157151, 819)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = X_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "\n",
    "def prepare_torch_dataset(batch_size, X, y):\n",
    "    tensor_x = torch.Tensor(X)\n",
    "    tensor_y = torch.Tensor(y).type(torch.LongTensor)\n",
    "\n",
    "    dataset = TensorDataset(tensor_x, tensor_y)\n",
    "    datasetloader = DataLoader(dataset, batch_size=batch_size)\n",
    "    return dataset, datasetloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "trainset, train_loader = prepare_torch_dataset(batch_size, X_train, y_train)\n",
    "\n",
    "valset, val_loader = prepare_torch_dataset(batch_size, X_valid, y_valid)\n",
    "\n",
    "testset, test_loader = prepare_torch_dataset(batch_size, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "\n",
    "input_size = n_features\n",
    "hidden_sizes = [1024, 512, 64]\n",
    "output_size = n_classes\n",
    "\n",
    "net = Sequential(\n",
    "    nn.Linear(input_size, hidden_sizes[0]),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(hidden_sizes[0], hidden_sizes[1]),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(hidden_sizes[1], hidden_sizes[2]),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(hidden_sizes[2], output_size),\n",
    "    nn.Softmax(dim=1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=819, out_features=1024, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=1024, out_features=512, bias=True)\n",
      "  (3): ReLU()\n",
      "  (4): Linear(in_features=512, out_features=64, bias=True)\n",
      "  (5): ReLU()\n",
      "  (6): Linear(in_features=64, out_features=7, bias=True)\n",
      "  (7): Softmax(dim=1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Loss function and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "train_loss: 1.711 - train_accuracy: 0.446  - val_loss: 1.633 - val_accuracy: 0.527\n",
      "Epoch 2/30\n",
      "train_loss: 1.608 - train_accuracy: 0.552  - val_loss: 1.588 - val_accuracy: 0.573\n",
      "Epoch 3/30\n",
      "train_loss: 1.567 - train_accuracy: 0.593  - val_loss: 1.560 - val_accuracy: 0.601\n",
      "Epoch 4/30\n",
      "train_loss: 1.541 - train_accuracy: 0.620  - val_loss: 1.537 - val_accuracy: 0.624\n",
      "Epoch 5/30\n",
      "train_loss: 1.521 - train_accuracy: 0.640  - val_loss: 1.520 - val_accuracy: 0.642\n",
      "Epoch 6/30\n",
      "train_loss: 1.505 - train_accuracy: 0.657  - val_loss: 1.504 - val_accuracy: 0.657\n",
      "Epoch 7/30\n",
      "train_loss: 1.491 - train_accuracy: 0.671  - val_loss: 1.492 - val_accuracy: 0.671\n",
      "Epoch 8/30\n",
      "train_loss: 1.480 - train_accuracy: 0.682  - val_loss: 1.486 - val_accuracy: 0.676\n",
      "Epoch 9/30\n",
      "train_loss: 1.470 - train_accuracy: 0.692  - val_loss: 1.479 - val_accuracy: 0.683\n",
      "Epoch 10/30\n",
      "train_loss: 1.462 - train_accuracy: 0.700  - val_loss: 1.472 - val_accuracy: 0.689\n",
      "Epoch 11/30\n",
      "train_loss: 1.455 - train_accuracy: 0.708  - val_loss: 1.467 - val_accuracy: 0.694\n",
      "Epoch 12/30\n",
      "train_loss: 1.449 - train_accuracy: 0.714  - val_loss: 1.461 - val_accuracy: 0.700\n",
      "Epoch 13/30\n",
      "train_loss: 1.443 - train_accuracy: 0.720  - val_loss: 1.453 - val_accuracy: 0.709\n",
      "Epoch 14/30\n",
      "train_loss: 1.439 - train_accuracy: 0.724  - val_loss: 1.452 - val_accuracy: 0.709\n",
      "Epoch 15/30\n",
      "train_loss: 1.435 - train_accuracy: 0.728  - val_loss: 1.441 - val_accuracy: 0.721\n",
      "Epoch 16/30\n",
      "train_loss: 1.432 - train_accuracy: 0.731  - val_loss: 1.448 - val_accuracy: 0.715\n",
      "Epoch 17/30\n",
      "train_loss: 1.428 - train_accuracy: 0.734  - val_loss: 1.440 - val_accuracy: 0.723\n",
      "Epoch 18/30\n",
      "train_loss: 1.427 - train_accuracy: 0.736  - val_loss: 1.445 - val_accuracy: 0.718\n",
      "Epoch 19/30\n",
      "train_loss: 1.424 - train_accuracy: 0.739  - val_loss: 1.448 - val_accuracy: 0.715\n",
      "Epoch 20/30\n",
      "train_loss: 1.424 - train_accuracy: 0.738  - val_loss: 1.449 - val_accuracy: 0.714\n",
      "Epoch 21/30\n",
      "train_loss: 1.424 - train_accuracy: 0.739  - val_loss: 1.441 - val_accuracy: 0.722\n",
      "Epoch 22/30\n",
      "train_loss: 1.423 - train_accuracy: 0.740  - val_loss: 1.443 - val_accuracy: 0.719\n",
      "Epoch 23/30\n",
      "train_loss: 1.424 - train_accuracy: 0.739  - val_loss: 1.441 - val_accuracy: 0.722\n",
      "Epoch 24/30\n",
      "train_loss: 1.425 - train_accuracy: 0.738  - val_loss: 1.442 - val_accuracy: 0.721\n",
      "Epoch 25/30\n",
      "train_loss: 1.428 - train_accuracy: 0.735  - val_loss: 1.452 - val_accuracy: 0.711\n",
      "Epoch 26/30\n",
      "train_loss: 1.431 - train_accuracy: 0.732  - val_loss: 1.443 - val_accuracy: 0.720\n",
      "Epoch 27/30\n",
      "train_loss: 1.432 - train_accuracy: 0.731  - val_loss: 1.451 - val_accuracy: 0.712\n",
      "Epoch 28/30\n",
      "train_loss: 1.438 - train_accuracy: 0.725  - val_loss: 1.463 - val_accuracy: 0.700\n",
      "Epoch 29/30\n",
      "train_loss: 1.443 - train_accuracy: 0.721  - val_loss: 1.460 - val_accuracy: 0.704\n",
      "Epoch 30/30\n",
      "train_loss: 1.449 - train_accuracy: 0.714  - val_loss: 1.464 - val_accuracy: 0.699\n",
      "Finished Training\n",
      "CPU times: user 6h 17min 7s, sys: 2min 4s, total: 6h 19min 11s\n",
      "Wall time: 1h 35min 21s\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 30\n",
    "\n",
    "%time net.fit(train_loader, val_loader, criterion, optimizer, n_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pandas as pd\n",
    "#import matplotlib.pyplot as plt\n",
    "\n",
    "#pd.DataFrame(history.history).plot(figsize=(8, 5))\n",
    "#plt.grid(True)\n",
    "#plt.gca().set_ylim(0, 1)\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save('models/tensorflow_keras/ravdess_savee_tess_emovo--10_10--StandardScaler--300_relu-100_relu-10_softmax--scc_sgd_accuracy--30')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
